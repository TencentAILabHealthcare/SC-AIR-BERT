import torch.nn as nn
from .token import TokenEmbedding
from .position import PositionalEmbedding
from .segment import SegmentEmbedding
import torch

# embedding_mode kmer(1mer、2mer、3mer)-"normal" 
class BERTEmbedding(nn.Module):
    """
    BERT Embedding which is consisted with under features
        1. TokenEmbedding : normal embedding matrix
        2. PositionalEmbedding : adding positional information using sin, cos
        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)

        sum of all these features are output of BERTEmbedding
    """

    def __init__(self, vocab_size, embed_size,embedding_mode, dropout=0.1,max_len=512):
        # print('vocab_size:',vocab_size)
        # print('embed_size:',embed_size)
        # print('max_len:',max_len)
        """
        :param vocab_size: total vocab size
        :param embed_size: embedding size of token embedding
        :param dropout: dropout rate
        """
        super().__init__()
        
        kidera =[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [-1.56 ,-1.67 ,-0.97 ,-0.27 ,-0.93 ,-0.78 ,-0.2 ,-0.08 ,0.21 ,-0.48],
        [0.22 ,1.27 ,1.37 ,1.87 ,-1.7 ,0.46 ,0.92 ,-0.39 ,0.23 ,0.93],
        [1.14 ,-0.07 ,-0.12 ,0.81 ,0.18 ,0.37 ,-0.09 ,1.23 ,1.1 ,-1.73 ],
        [0.58 ,-0.22 ,-1.58 ,0.81 ,-0.92 ,0.15 ,-1.52 ,0.47 ,0.76 ,0.7 ],
        [0.12 ,-0.89 ,0.45 ,-1.05 ,-0.71 ,2.41 ,1.52 ,-0.69 ,1.13 ,1.1 ],
        [-0.47 ,0.24 ,0.07 ,1.1 ,1.1 ,0.59 ,0.84 ,-0.71 ,-0.03 ,-2.33 ],
        [-1.45 ,0.19 ,-1.61 ,1.17 ,-1.31 ,0.4 ,0.04 ,0.38 ,-0.35 ,-0.12], 
        [1.46 ,-1.96 ,-0.23 ,-0.16 ,0.1 ,-0.11 ,1.32 ,2.36 ,-1.66 ,0.46 ],
        [-0.41 ,0.52 ,-0.28 ,0.28 ,1.61 ,1.01 ,-1.85 ,0.47 ,1.13 ,1.63 ],
        [-0.73 ,-0.16 ,1.79 ,-0.77 ,-0.54 ,0.03 ,-0.83 ,0.51 ,0.66 ,-1.78 ],
        [-1.04 ,0 ,-0.24 ,-1.1 ,-0.55 ,-2.05 ,0.96 ,-0.76 ,0.45 ,0.93 ],
        [-0.34 ,0.82 ,-0.23 ,1.7 ,1.54 ,-1.62 ,1.15 ,-0.08 ,-0.48 ,0.6 ],
        [-1.4 ,0.18 ,-0.42 ,-0.73 ,2 ,1.52 ,0.26 ,0.11 ,-1.27 ,0.27 ],
        [-0.21 ,0.98 ,-0.36 ,-1.43 ,0.22 ,-0.81 ,0.67 ,1.1 ,1.71 ,-0.44 ],
        [2.06 ,-0.33 ,-1.15 ,-0.75 ,0.88 ,-0.45 ,0.3 ,-2.3 ,0.74 ,-0.28 ],
        [0.81 ,-1.08 ,0.16 ,0.42 ,-0.21 ,-0.43 ,-1.89 ,-1.15 ,-0.97 ,-0.23], 
        [0.26 ,-0.7 ,1.21 ,0.63 ,-0.1 ,0.21 ,0.24 ,-1.15 ,-0.56 ,0.19 ],
        [0.3 ,2.1 ,-0.72 ,-1.57 ,-1.16 ,0.57 ,-0.48 ,-0.4 ,-2.3 ,-0.6 ],
        [1.38 ,1.48 ,0.8 ,-0.56 ,0 ,-0.68 ,-0.31 ,1.03 ,-0.05 ,0.53 ],
        [-0.74 ,-0.71 ,2.04 ,-0.4 ,0.5 ,-0.81 ,-1.07 ,0.06 ,-0.46 ,0.65]]

        atchley = [[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [-0.591, -1.302, -0.733, 1.570, -0.146,-0.591, -1.302, -0.733, 1.570, -0.146],
        [-1.343, 0.465, -0.862, -1.020, -0.255,-1.343, 0.465, -0.862, -1.020, -0.255,],
        [1.050, 0.302, -3.656, -0.259, -3.242,1.050, 0.302, -3.656, -0.259, -3.242],
        [1.357, -1.453, 1.477, 0.113, -0.837,1.357, -1.453, 1.477, 0.113, -0.837],
        [-1.006, -0.590, 1.891, -0.397, 0.412,-1.006, -0.590, 1.891, -0.397, 0.412],
        [-0.384, 1.652, 1.330, 1.045, 2.064,-0.384, 1.652, 1.330, 1.045, 2.064],
        [0.336, -0.417, -1.673, -1.474, -0.078,0.336, -0.417, -1.673, -1.474, -0.078],
        [-1.239, -0.547, 2.131, 0.393, 0.816,-1.239, -0.547, 2.131, 0.393, 0.816],
        [1.831, -0.561, 0.533, -0.277, 1.648,1.831, -0.561, 0.533, -0.277, 1.648],
        [-1.019, -0.987, -1.505, 1.266, -0.912,-1.019, -0.987, -1.505, 1.266, -0.912],
        [-0.663, -1.524, 2.219, -1.005, 1.212,-0.663, -1.524, 2.219, -1.005, 1.212],
        [0.945, 0.828, 1.299, -0.169, 0.933,0.945, 0.828, 1.299, -0.169, 0.933],
        [0.189, 2.081, -1.628, 0.421, -1.392,0.189, 2.081, -1.628, 0.421, -1.392],
        [0.931, -0.179, -3.005, -0.503, -1.853,0.931, -0.179, -3.005, -0.503, -1.853],
        [1.538, -0.055, 1.502, 0.440, 2.897,1.538, -0.055, 1.502, 0.440, 2.897],
        [-0.228, 1.399, -4.760, 0.670, -2.647,-0.228, 1.399, -4.760, 0.670, -2.647],
        [-0.032, 0.326, 2.213, 0.908, 1.313,-0.032, 0.326, 2.213, 0.908, 1.313],
        [-1.337, -0.279, -0.544, 1.242, -1.262,-1.337, -0.279, -0.544, 1.242, -1.262],
        [-0.595, 0.009, 0.672, -2.128, -0.184,-0.595, 0.009, 0.672, -2.128, -0.184],
        [0.260, 0.830, 3.097, -0.838, 1.512,0.260, 0.830, 3.097, -0.838, 1.512]]

        one_hot = [
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0],
        [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0]]

        if(embedding_mode=='normal'):
            self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)
        else:
            if(embedding_mode=='atchley'):
                weight = torch.FloatTensor(atchley)
            elif(embedding_mode=='kidera'):
                weight = torch.FloatTensor(kidera)
            elif(embedding_mode=='onehot'):
                weight = torch.FloatTensor(one_hot)
            self.token = nn.Embedding.from_pretrained(weight,padding_idx=0)

        self.position = PositionalEmbedding(d_model=self.token.embedding_dim,max_len=max_len)
        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)

        # print('token:',self.token)
        # print('position:',self.position.shape)
        # print('segment:',self.segment.shape)

        self.dropout = nn.Dropout(p=dropout)
        self.embed_size = embed_size

    def forward(self, sequence, segment_label):
        # print('self.token(sequence):',self.token(sequence))
        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)

        return self.dropout(x)
